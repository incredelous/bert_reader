{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T13:41:50.677572Z",
     "start_time": "2018-12-18T13:41:48.654388Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tokenization\n",
    "from run_squad import SquadExample, convert_examples_to_features, FeatureWriter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T13:41:50.763458Z",
     "start_time": "2018-12-18T13:41:50.679865Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file='./checkpoints/chinese_L-12_H-768_A-12/vocab.txt', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T13:41:50.784504Z",
     "start_time": "2018-12-18T13:41:50.766234Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_squad_examples(input_file, is_training):\n",
    "  \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "  with tf.gfile.Open(input_file, \"r\") as reader:\n",
    "    input_data = json.load(reader)[\"data\"]\n",
    "\n",
    "  def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "      return True\n",
    "    return False\n",
    "\n",
    "  examples = []\n",
    "  for entry in input_data:\n",
    "    for paragraph in entry[\"paragraphs\"]:\n",
    "      paragraph_text = paragraph[\"context\"]\n",
    "      doc_tokens = []\n",
    "      char_to_word_offset = []\n",
    "      prev_is_whitespace = True\n",
    "      for c in paragraph_text:\n",
    "        if is_whitespace(c):\n",
    "          prev_is_whitespace = True\n",
    "        else:\n",
    "          if prev_is_whitespace:\n",
    "            doc_tokens.append(c)\n",
    "          else:\n",
    "            doc_tokens[-1] += c\n",
    "          prev_is_whitespace = False\n",
    "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "      for qa in paragraph[\"qas\"]:\n",
    "        qas_id = qa[\"id\"]\n",
    "        question_text = qa[\"question\"]\n",
    "        start_position = None\n",
    "        end_position = None\n",
    "        orig_answer_text = None\n",
    "        is_impossible = False\n",
    "        if is_training:\n",
    "\n",
    "          if False:\n",
    "            is_impossible = qa[\"is_impossible\"]\n",
    "          if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
    "            raise ValueError(\n",
    "                \"For training, each question should have exactly 1 answer.\")\n",
    "          if not is_impossible:\n",
    "            answer = qa[\"answers\"][0]\n",
    "            orig_answer_text = answer[\"text\"]\n",
    "            answer_offset = answer[\"answer_start\"]\n",
    "            answer_length = len(orig_answer_text)\n",
    "            start_position = char_to_word_offset[answer_offset]\n",
    "            end_position = char_to_word_offset[answer_offset + answer_length -\n",
    "                                               1]\n",
    "            # Only add answers where the text can be exactly recovered from the\n",
    "            # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "            # stuff so we will just skip the example.\n",
    "            #\n",
    "            # Note that this means for training mode, every example is NOT\n",
    "            # guaranteed to be preserved.\n",
    "            actual_text = \" \".join(\n",
    "                doc_tokens[start_position:(end_position + 1)])\n",
    "            cleaned_answer_text = \" \".join(\n",
    "                tokenization.whitespace_tokenize(orig_answer_text))\n",
    "            if actual_text.find(cleaned_answer_text) == -1:\n",
    "              tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "                                 actual_text, cleaned_answer_text)\n",
    "              continue\n",
    "          else:\n",
    "            start_position = -1\n",
    "            end_position = -1\n",
    "            orig_answer_text = \"\"\n",
    "        \n",
    "        example = SquadExample(\n",
    "            qas_id=qas_id,\n",
    "            question_text=question_text,\n",
    "            doc_tokens=doc_tokens,\n",
    "            orig_answer_text=orig_answer_text,\n",
    "            start_position=start_position,\n",
    "            end_position=end_position,\n",
    "            is_impossible=is_impossible)\n",
    "        examples.append(example)\n",
    "\n",
    "  return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-18T13:42:00.044645Z",
     "start_time": "2018-12-18T13:41:50.786651Z"
    }
   },
   "outputs": [],
   "source": [
    "input_file = './data/SQuAD1.1/train-v1.1.json'\n",
    "train_examples = read_squad_examples(input_file, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T07:15:09.469709Z",
     "start_time": "2018-12-18T13:42:00.047965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /root/workspace/bert_reader/run_squad.py(317)convert_examples_to_features()\n",
      "-> for (example_index, example) in enumerate(examples):\n",
      "(Pdb) len(examples)\n",
      "87599\n",
      "(Pdb) examples[0]\n",
      "qas_id: 5733be284776f41900661182, question_text: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?, doc_tokens: [Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.], start_position: 90, end_position: 92, is_impossible: False\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(318)convert_examples_to_features()\n",
      "-> query_tokens = tokenizer.tokenize(example.question_text)\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(320)convert_examples_to_features()\n",
      "-> if len(query_tokens) > max_query_length:\n",
      "(Pdb) query_tokens\n",
      "['to', 'who', '##m', 'di', '##d', 'the', 'vi', '##r', '##gin', 'mary', 'all', '##eg', '##ed', '##ly', 'app', '##ear', 'in', '185', '##8', 'in', 'lo', '##ur', '##des', 'france', '?']\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(323)convert_examples_to_features()\n",
      "-> tok_to_orig_index = []\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(324)convert_examples_to_features()\n",
      "-> orig_to_tok_index = []\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(325)convert_examples_to_features()\n",
      "-> all_doc_tokens = []\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(326)convert_examples_to_features()\n",
      "-> for (i, token) in enumerate(example.doc_tokens):\n",
      "(Pdb) h\n",
      "\n",
      "Documented commands (type help <topic>):\n",
      "========================================\n",
      "EOF    c          d        h         list      q        rv       undisplay\n",
      "a      cl         debug    help      ll        quit     s        unt      \n",
      "alias  clear      disable  ignore    longlist  r        source   until    \n",
      "args   commands   display  interact  n         restart  step     up       \n",
      "b      condition  down     j         next      return   tbreak   w        \n",
      "break  cont       enable   jump      p         retval   u        whatis   \n",
      "bt     continue   exit     l         pp        run      unalias  where    \n",
      "\n",
      "Miscellaneous help topics:\n",
      "==========================\n",
      "exec  pdb\n",
      "\n",
      "(Pdb) until 333\n",
      "> /root/workspace/bert_reader/run_squad.py(333)convert_examples_to_features()\n",
      "-> tok_start_position = None\n",
      "(Pdb) tok_to_orig_index\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 8, 9, 10, 10, 10, 10, 11, 12, 12, 13, 14, 15, 16, 16, 16, 17, 18, 19, 19, 19, 20, 20, 21, 21, 21, 21, 22, 23, 23, 23, 24, 25, 26, 27, 27, 28, 29, 29, 30, 30, 31, 32, 33, 33, 34, 34, 34, 35, 36, 36, 37, 38, 38, 39, 39, 39, 39, 40, 41, 42, 42, 42, 43, 43, 43, 43, 44, 45, 46, 46, 46, 46, 46, 47, 48, 49, 50, 51, 51, 52, 53, 54, 54, 54, 55, 56, 57, 57, 57, 58, 58, 59, 59, 59, 59, 60, 60, 60, 61, 62, 62, 62, 63, 64, 65, 65, 65, 65, 66, 67, 67, 68, 69, 70, 70, 70, 71, 72, 72, 72, 72, 72, 73, 74, 75, 76, 76, 76, 77, 78, 79, 79, 79, 80, 81, 81, 81, 81, 82, 83, 84, 85, 85, 85, 86, 87, 87, 87, 87, 88, 88, 88, 89, 90, 91, 91, 91, 91, 92, 92, 92, 92, 93, 94, 94, 94, 95, 96, 97, 98, 99, 100, 101, 102, 102, 103, 104, 105, 105, 106, 107, 108, 108, 109, 109, 109, 110, 111, 111, 111, 111, 112, 113, 114, 115, 115, 115, 115, 116, 117, 118, 118, 119, 120, 121, 121, 121, 122, 123, 123]\n",
      "(Pdb) orig_to_tok_index\n",
      "[0, 7, 8, 9, 10, 11, 14, 19, 21, 22, 23, 27, 28, 30, 31, 32, 33, 36, 37, 38, 41, 43, 47, 48, 51, 52, 53, 54, 56, 57, 59, 61, 62, 63, 65, 68, 69, 71, 72, 74, 78, 79, 80, 83, 87, 88, 89, 94, 95, 96, 97, 98, 100, 101, 102, 105, 106, 107, 110, 112, 116, 119, 120, 123, 124, 125, 129, 130, 132, 133, 134, 137, 138, 143, 144, 145, 146, 149, 150, 151, 154, 155, 159, 160, 161, 162, 165, 166, 170, 173, 174, 175, 179, 183, 184, 187, 188, 189, 190, 191, 192, 193, 194, 196, 197, 198, 200, 201, 202, 204, 207, 208, 212, 213, 214, 215, 219, 220, 221, 223, 224, 225, 228, 229]\n",
      "(Pdb) all_doc_tokens\n",
      "['ar', '##chi', '##te', '##ct', '##ura', '##lly', ',', 'the', 'school', 'has', 'a', 'cat', '##ho', '##lic', 'ch', '##ara', '##ct', '##er', '.', 'at', '##op', 'the', 'main', 'build', '##ing', \"'\", 's', 'gold', 'dom', '##e', 'is', 'a', 'golden', 'st', '##at', '##ue', 'of', 'the', 'vi', '##r', '##gin', 'mary', '.', 'im', '##media', '##tel', '##y', 'in', 'f', '##ron', '##t', 'of', 'the', 'main', 'build', '##ing', 'and', 'fa', '##cing', 'it', ',', 'is', 'a', 'co', '##pper', 'st', '##at', '##ue', 'of', 'chris', '##t', 'with', 'arm', '##s', 'up', '##ra', '##ise', '##d', 'with', 'the', 'le', '##gen', '##d', '\"', 've', '##ni', '##te', 'ad', 'me', 'o', '##mn', '##es', '\"', '.', 'next', 'to', 'the', 'main', 'build', '##ing', 'is', 'the', 'bas', '##il', '##ica', 'of', 'the', 'sa', '##cr', '##ed', 'heart', '.', 'im', '##media', '##tel', '##y', 'be', '##hin', '##d', 'the', 'bas', '##il', '##ica', 'is', 'the', 'g', '##ro', '##tto', ',', 'a', 'maria', '##n', 'place', 'of', 'pr', '##ay', '##er', 'and', 're', '##fl', '##ect', '##ion', '.', 'it', 'is', 'a', 're', '##pl', '##ica', 'of', 'the', 'g', '##ro', '##tto', 'at', 'lo', '##ur', '##des', ',', 'france', 'where', 'the', 'vi', '##r', '##gin', 'mary', 're', '##put', '##ed', '##ly', 'app', '##ear', '##ed', 'to', 'saint', 'be', '##rn', '##ade', '##tte', 'so', '##ub', '##ir', '##ous', 'in', '185', '##8', '.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'di', '##rect', 'line', 'that', 'connect', '##s', 't', '##hr', '##ough', '3', 'st', '##at', '##ue', '##s', 'and', 'the', 'gold', 'dom', '##e', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'st', '##at', '##ue', 'of', 'mary', '.']\n",
      "(Pdb) example.doc_tokens\n",
      "['Architecturally,', 'the', 'school', 'has', 'a', 'Catholic', 'character.', 'Atop', 'the', 'Main', \"Building's\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'Virgin', 'Mary.', 'Immediately', 'in', 'front', 'of', 'the', 'Main', 'Building', 'and', 'facing', 'it,', 'is', 'a', 'copper', 'statue', 'of', 'Christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '\"Venite', 'Ad', 'Me', 'Omnes\".', 'Next', 'to', 'the', 'Main', 'Building', 'is', 'the', 'Basilica', 'of', 'the', 'Sacred', 'Heart.', 'Immediately', 'behind', 'the', 'basilica', 'is', 'the', 'Grotto,', 'a', 'Marian', 'place', 'of', 'prayer', 'and', 'reflection.', 'It', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'Lourdes,', 'France', 'where', 'the', 'Virgin', 'Mary', 'reputedly', 'appeared', 'to', 'Saint', 'Bernadette', 'Soubirous', 'in', '1858.', 'At', 'the', 'end', 'of', 'the', 'main', 'drive', '(and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'Gold', 'Dome),', 'is', 'a', 'simple,', 'modern', 'stone', 'statue', 'of', 'Mary.']\n",
      "(Pdb) until 352\n",
      "> /root/workspace/bert_reader/run_squad.py(354)convert_examples_to_features()\n",
      "-> _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
      "(Pdb) until 370\n",
      "> /root/workspace/bert_reader/run_squad.py(370)convert_examples_to_features()\n",
      "-> token_is_max_context = {}\n",
      "(Pdb) _DocSpan\n",
      "<class 'run_squad.DocSpan'>\n",
      "(Pdb) _DocSpan['DocSpan']\n",
      "*** TypeError: 'type' object is not subscriptable\n",
      "(Pdb) doc_spans\n",
      "[DocSpan(start=0, length=231)]\n",
      "(Pdb) doc_spans[0]\n",
      "DocSpan(start=0, length=231)\n",
      "(Pdb) doc_spans[0]['start']\n",
      "*** TypeError: tuple indices must be integers or slices, not str\n",
      "(Pdb) doc_spans[0].start\n",
      "0\n",
      "(Pdb) doc_spans[0].length\n",
      "231\n",
      "(Pdb) dir(doc_spans[0])\n",
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '_asdict', '_fields', '_make', '_replace', '_source', 'count', 'index', 'length', 'start']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) doc_spans[0].count\n",
      "<built-in method count of DocSpan object at 0x7f3966f96318>\n",
      "(Pdb) doc_spans[0].count()\n",
      "*** TypeError: count() takes exactly one argument (0 given)\n",
      "(Pdb) doc_spans[0].index\n",
      "<built-in method index of DocSpan object at 0x7f3966f96318>\n",
      "(Pdb) start_offset\n",
      "0\n",
      "(Pdb) until 396\n",
      "> /root/workspace/bert_reader/run_squad.py(397)convert_examples_to_features()\n",
      "-> input_mask = [1] * len(input_ids)\n",
      "(Pdb) segment_ids\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "(Pdb) token_to_orgi_map\n",
      "*** NameError: name 'token_to_orgi_map' is not defined\n",
      "(Pdb) token_to_orig_map\n",
      "{27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 1, 35: 2, 36: 3, 37: 4, 38: 5, 39: 5, 40: 5, 41: 6, 42: 6, 43: 6, 44: 6, 45: 6, 46: 7, 47: 7, 48: 8, 49: 9, 50: 10, 51: 10, 52: 10, 53: 10, 54: 11, 55: 12, 56: 12, 57: 13, 58: 14, 59: 15, 60: 16, 61: 16, 62: 16, 63: 17, 64: 18, 65: 19, 66: 19, 67: 19, 68: 20, 69: 20, 70: 21, 71: 21, 72: 21, 73: 21, 74: 22, 75: 23, 76: 23, 77: 23, 78: 24, 79: 25, 80: 26, 81: 27, 82: 27, 83: 28, 84: 29, 85: 29, 86: 30, 87: 30, 88: 31, 89: 32, 90: 33, 91: 33, 92: 34, 93: 34, 94: 34, 95: 35, 96: 36, 97: 36, 98: 37, 99: 38, 100: 38, 101: 39, 102: 39, 103: 39, 104: 39, 105: 40, 106: 41, 107: 42, 108: 42, 109: 42, 110: 43, 111: 43, 112: 43, 113: 43, 114: 44, 115: 45, 116: 46, 117: 46, 118: 46, 119: 46, 120: 46, 121: 47, 122: 48, 123: 49, 124: 50, 125: 51, 126: 51, 127: 52, 128: 53, 129: 54, 130: 54, 131: 54, 132: 55, 133: 56, 134: 57, 135: 57, 136: 57, 137: 58, 138: 58, 139: 59, 140: 59, 141: 59, 142: 59, 143: 60, 144: 60, 145: 60, 146: 61, 147: 62, 148: 62, 149: 62, 150: 63, 151: 64, 152: 65, 153: 65, 154: 65, 155: 65, 156: 66, 157: 67, 158: 67, 159: 68, 160: 69, 161: 70, 162: 70, 163: 70, 164: 71, 165: 72, 166: 72, 167: 72, 168: 72, 169: 72, 170: 73, 171: 74, 172: 75, 173: 76, 174: 76, 175: 76, 176: 77, 177: 78, 178: 79, 179: 79, 180: 79, 181: 80, 182: 81, 183: 81, 184: 81, 185: 81, 186: 82, 187: 83, 188: 84, 189: 85, 190: 85, 191: 85, 192: 86, 193: 87, 194: 87, 195: 87, 196: 87, 197: 88, 198: 88, 199: 88, 200: 89, 201: 90, 202: 91, 203: 91, 204: 91, 205: 91, 206: 92, 207: 92, 208: 92, 209: 92, 210: 93, 211: 94, 212: 94, 213: 94, 214: 95, 215: 96, 216: 97, 217: 98, 218: 99, 219: 100, 220: 101, 221: 102, 222: 102, 223: 103, 224: 104, 225: 105, 226: 105, 227: 106, 228: 107, 229: 108, 230: 108, 231: 109, 232: 109, 233: 109, 234: 110, 235: 111, 236: 111, 237: 111, 238: 111, 239: 112, 240: 113, 241: 114, 242: 115, 243: 115, 244: 115, 245: 115, 246: 116, 247: 117, 248: 118, 249: 118, 250: 119, 251: 120, 252: 121, 253: 121, 254: 121, 255: 122, 256: 123, 257: 123}\n",
      "(Pdb) tok_to_orig_index\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 8, 9, 10, 10, 10, 10, 11, 12, 12, 13, 14, 15, 16, 16, 16, 17, 18, 19, 19, 19, 20, 20, 21, 21, 21, 21, 22, 23, 23, 23, 24, 25, 26, 27, 27, 28, 29, 29, 30, 30, 31, 32, 33, 33, 34, 34, 34, 35, 36, 36, 37, 38, 38, 39, 39, 39, 39, 40, 41, 42, 42, 42, 43, 43, 43, 43, 44, 45, 46, 46, 46, 46, 46, 47, 48, 49, 50, 51, 51, 52, 53, 54, 54, 54, 55, 56, 57, 57, 57, 58, 58, 59, 59, 59, 59, 60, 60, 60, 61, 62, 62, 62, 63, 64, 65, 65, 65, 65, 66, 67, 67, 68, 69, 70, 70, 70, 71, 72, 72, 72, 72, 72, 73, 74, 75, 76, 76, 76, 77, 78, 79, 79, 79, 80, 81, 81, 81, 81, 82, 83, 84, 85, 85, 85, 86, 87, 87, 87, 87, 88, 88, 88, 89, 90, 91, 91, 91, 91, 92, 92, 92, 92, 93, 94, 94, 94, 95, 96, 97, 98, 99, 100, 101, 102, 102, 103, 104, 105, 105, 106, 107, 108, 108, 109, 109, 109, 110, 111, 111, 111, 111, 112, 113, 114, 115, 115, 115, 115, 116, 117, 118, 118, 119, 120, 121, 121, 121, 122, 123, 123]\n",
      "(Pdb) tokens\n",
      "['[CLS]', 'to', 'who', '##m', 'di', '##d', 'the', 'vi', '##r', '##gin', 'mary', 'all', '##eg', '##ed', '##ly', 'app', '##ear', 'in', '185', '##8', 'in', 'lo', '##ur', '##des', 'france', '?', '[SEP]', 'ar', '##chi', '##te', '##ct', '##ura', '##lly', ',', 'the', 'school', 'has', 'a', 'cat', '##ho', '##lic', 'ch', '##ara', '##ct', '##er', '.', 'at', '##op', 'the', 'main', 'build', '##ing', \"'\", 's', 'gold', 'dom', '##e', 'is', 'a', 'golden', 'st', '##at', '##ue', 'of', 'the', 'vi', '##r', '##gin', 'mary', '.', 'im', '##media', '##tel', '##y', 'in', 'f', '##ron', '##t', 'of', 'the', 'main', 'build', '##ing', 'and', 'fa', '##cing', 'it', ',', 'is', 'a', 'co', '##pper', 'st', '##at', '##ue', 'of', 'chris', '##t', 'with', 'arm', '##s', 'up', '##ra', '##ise', '##d', 'with', 'the', 'le', '##gen', '##d', '\"', 've', '##ni', '##te', 'ad', 'me', 'o', '##mn', '##es', '\"', '.', 'next', 'to', 'the', 'main', 'build', '##ing', 'is', 'the', 'bas', '##il', '##ica', 'of', 'the', 'sa', '##cr', '##ed', 'heart', '.', 'im', '##media', '##tel', '##y', 'be', '##hin', '##d', 'the', 'bas', '##il', '##ica', 'is', 'the', 'g', '##ro', '##tto', ',', 'a', 'maria', '##n', 'place', 'of', 'pr', '##ay', '##er', 'and', 're', '##fl', '##ect', '##ion', '.', 'it', 'is', 'a', 're', '##pl', '##ica', 'of', 'the', 'g', '##ro', '##tto', 'at', 'lo', '##ur', '##des', ',', 'france', 'where', 'the', 'vi', '##r', '##gin', 'mary', 're', '##put', '##ed', '##ly', 'app', '##ear', '##ed', 'to', 'saint', 'be', '##rn', '##ade', '##tte', 'so', '##ub', '##ir', '##ous', 'in', '185', '##8', '.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'di', '##rect', 'line', 'that', 'connect', '##s', 't', '##hr', '##ough', '3', 'st', '##at', '##ue', '##s', 'and', 'the', 'gold', 'dom', '##e', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'st', '##at', '##ue', 'of', 'mary', '.', '[SEP]']\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(400)convert_examples_to_features()\n",
      "-> while len(input_ids) < max_seq_length:\n",
      "(Pdb) until 408\n",
      "> /root/workspace/bert_reader/run_squad.py(409)convert_examples_to_features()\n",
      "-> start_position = None\n",
      "(Pdb) segment_ids\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "(Pdb) input_ids\n",
      "[101, 8228, 9372, 8175, 9796, 8168, 8174, 10138, 8180, 10822, 10786, 8513, 10935, 8303, 8436, 8172, 11979, 8217, 9560, 8156, 8217, 12264, 8685, 11211, 11653, 136, 102, 8673, 10525, 8299, 8722, 10238, 9456, 117, 8174, 9467, 11325, 143, 10165, 9570, 11809, 9537, 10225, 8722, 8196, 119, 8243, 9133, 8174, 9139, 11561, 8221, 112, 161, 10628, 11183, 8154, 8310, 143, 12746, 8811, 8488, 8803, 8205, 8174, 10138, 8180, 10822, 10786, 119, 10481, 10970, 11246, 8179, 8217, 148, 8991, 8165, 8205, 8174, 9139, 11561, 8221, 8256, 12289, 11540, 8233, 117, 8310, 143, 8792, 11413, 8811, 8488, 8803, 8205, 9582, 8165, 8663, 9498, 8118, 8644, 8332, 9651, 8168, 8663, 8174, 8983, 10152, 8168, 107, 12810, 8833, 8299, 9145, 8450, 157, 11919, 8333, 107, 119, 9321, 8228, 8174, 9139, 11561, 8221, 8310, 8174, 10952, 8742, 9156, 8205, 8174, 8945, 13129, 8303, 10240, 119, 10481, 10970, 11246, 8179, 8815, 13098, 8168, 8174, 10952, 8742, 9156, 8310, 8174, 149, 8607, 11403, 117, 143, 12096, 8171, 10963, 8205, 9470, 8760, 8196, 8256, 8847, 11690, 10862, 8410, 119, 8233, 8310, 143, 8847, 12569, 9156, 8205, 8174, 149, 8607, 11403, 8243, 12264, 8685, 11211, 117, 11653, 11703, 8174, 10138, 8180, 10822, 10786, 8847, 11300, 8303, 8436, 8172, 11979, 8303, 8228, 10484, 8815, 9256, 10233, 9786, 8968, 10544, 8977, 9822, 8217, 9560, 8156, 119, 8243, 8174, 9931, 8205, 8174, 9139, 10170, 113, 8256, 8217, 143, 9796, 13075, 8323, 9231, 12280, 8118, 162, 10561, 11600, 124, 8811, 8488, 8803, 8118, 8256, 8174, 10628, 11183, 8154, 114, 117, 8310, 143, 11996, 117, 12862, 11073, 8811, 8488, 8803, 8205, 10786, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Pdb) input_mask\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(410)convert_examples_to_features()\n",
      "-> end_position = None\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(411)convert_examples_to_features()\n",
      "-> if is_training and not example.is_impossible:\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(414)convert_examples_to_features()\n",
      "-> doc_start = doc_span.start\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(415)convert_examples_to_features()\n",
      "-> doc_end = doc_span.start + doc_span.length - 1\n",
      "(Pdb) n\n",
      "> /root/workspace/bert_reader/run_squad.py(416)convert_examples_to_features()\n",
      "-> out_of_span = False\n",
      "(Pdb) until 467\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:unique_id: 1000000000\n",
      "INFO:tensorflow:example_index: 0\n",
      "INFO:tensorflow:doc_span_index: 0\n",
      "INFO:tensorflow:tokens: [CLS] to who ##m di ##d the vi ##r ##gin mary all ##eg ##ed ##ly app ##ear in 185 ##8 in lo ##ur ##des france ? [SEP] ar ##chi ##te ##ct ##ura ##lly , the school has a cat ##ho ##lic ch ##ara ##ct ##er . at ##op the main build ##ing ' s gold dom ##e is a golden st ##at ##ue of the vi ##r ##gin mary . im ##media ##tel ##y in f ##ron ##t of the main build ##ing and fa ##cing it , is a co ##pper st ##at ##ue of chris ##t with arm ##s up ##ra ##ise ##d with the le ##gen ##d \" ve ##ni ##te ad me o ##mn ##es \" . next to the main build ##ing is the bas ##il ##ica of the sa ##cr ##ed heart . im ##media ##tel ##y be ##hin ##d the bas ##il ##ica is the g ##ro ##tto , a maria ##n place of pr ##ay ##er and re ##fl ##ect ##ion . it is a re ##pl ##ica of the g ##ro ##tto at lo ##ur ##des , france where the vi ##r ##gin mary re ##put ##ed ##ly app ##ear ##ed to saint be ##rn ##ade ##tte so ##ub ##ir ##ous in 185 ##8 . at the end of the main drive ( and in a di ##rect line that connect ##s t ##hr ##ough 3 st ##at ##ue ##s and the gold dom ##e ) , is a simple , modern stone st ##at ##ue of mary . [SEP]\n",
      "INFO:tensorflow:token_to_orig_map: 27:0 28:0 29:0 30:0 31:0 32:0 33:0 34:1 35:2 36:3 37:4 38:5 39:5 40:5 41:6 42:6 43:6 44:6 45:6 46:7 47:7 48:8 49:9 50:10 51:10 52:10 53:10 54:11 55:12 56:12 57:13 58:14 59:15 60:16 61:16 62:16 63:17 64:18 65:19 66:19 67:19 68:20 69:20 70:21 71:21 72:21 73:21 74:22 75:23 76:23 77:23 78:24 79:25 80:26 81:27 82:27 83:28 84:29 85:29 86:30 87:30 88:31 89:32 90:33 91:33 92:34 93:34 94:34 95:35 96:36 97:36 98:37 99:38 100:38 101:39 102:39 103:39 104:39 105:40 106:41 107:42 108:42 109:42 110:43 111:43 112:43 113:43 114:44 115:45 116:46 117:46 118:46 119:46 120:46 121:47 122:48 123:49 124:50 125:51 126:51 127:52 128:53 129:54 130:54 131:54 132:55 133:56 134:57 135:57 136:57 137:58 138:58 139:59 140:59 141:59 142:59 143:60 144:60 145:60 146:61 147:62 148:62 149:62 150:63 151:64 152:65 153:65 154:65 155:65 156:66 157:67 158:67 159:68 160:69 161:70 162:70 163:70 164:71 165:72 166:72 167:72 168:72 169:72 170:73 171:74 172:75 173:76 174:76 175:76 176:77 177:78 178:79 179:79 180:79 181:80 182:81 183:81 184:81 185:81 186:82 187:83 188:84 189:85 190:85 191:85 192:86 193:87 194:87 195:87 196:87 197:88 198:88 199:88 200:89 201:90 202:91 203:91 204:91 205:91 206:92 207:92 208:92 209:92 210:93 211:94 212:94 213:94 214:95 215:96 216:97 217:98 218:99 219:100 220:101 221:102 222:102 223:103 224:104 225:105 226:105 227:106 228:107 229:108 230:108 231:109 232:109 233:109 234:110 235:111 236:111 237:111 238:111 239:112 240:113 241:114 242:115 243:115 244:115 245:115 246:116 247:117 248:118 249:118 250:119 251:120 252:121 253:121 254:121 255:122 256:123 257:123\n",
      "INFO:tensorflow:token_is_max_context: 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True\n",
      "INFO:tensorflow:input_ids: 101 8228 9372 8175 9796 8168 8174 10138 8180 10822 10786 8513 10935 8303 8436 8172 11979 8217 9560 8156 8217 12264 8685 11211 11653 136 102 8673 10525 8299 8722 10238 9456 117 8174 9467 11325 143 10165 9570 11809 9537 10225 8722 8196 119 8243 9133 8174 9139 11561 8221 112 161 10628 11183 8154 8310 143 12746 8811 8488 8803 8205 8174 10138 8180 10822 10786 119 10481 10970 11246 8179 8217 148 8991 8165 8205 8174 9139 11561 8221 8256 12289 11540 8233 117 8310 143 8792 11413 8811 8488 8803 8205 9582 8165 8663 9498 8118 8644 8332 9651 8168 8663 8174 8983 10152 8168 107 12810 8833 8299 9145 8450 157 11919 8333 107 119 9321 8228 8174 9139 11561 8221 8310 8174 10952 8742 9156 8205 8174 8945 13129 8303 10240 119 10481 10970 11246 8179 8815 13098 8168 8174 10952 8742 9156 8310 8174 149 8607 11403 117 143 12096 8171 10963 8205 9470 8760 8196 8256 8847 11690 10862 8410 119 8233 8310 143 8847 12569 9156 8205 8174 149 8607 11403 8243 12264 8685 11211 117 11653 11703 8174 10138 8180 10822 10786 8847 11300 8303 8436 8172 11979 8303 8228 10484 8815 9256 10233 9786 8968 10544 8977 9822 8217 9560 8156 119 8243 8174 9931 8205 8174 9139 10170 113 8256 8217 143 9796 13075 8323 9231 12280 8118 162 10561 11600 124 8811 8488 8803 8118 8256 8174 10628 11183 8154 114 117 8310 143 11996 117 12862 11073 8811 8488 8803 8205 10786 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:start_position: 201\n",
      "INFO:tensorflow:end_position: 209\n",
      "INFO:tensorflow:answer: saint be ##rn ##ade ##tte so ##ub ##ir ##ous\n",
      "> /root/workspace/bert_reader/run_squad.py(467)convert_examples_to_features()\n",
      "-> input_ids=input_ids,\n",
      "(Pdb) doc_spans\n",
      "[DocSpan(start=0, length=231)]\n",
      "(Pdb) doc_spans[0]\n",
      "DocSpan(start=0, length=231)\n",
      "(Pdb) doc_spans[0].start\n",
      "0\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b0aa07c68bfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmax_query_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         output_fn=train_writer.process_feature)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mtrain_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/bert_reader/run_squad.py\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(examples, tokenizer, max_seq_length, doc_stride, max_query_length, is_training, output_fn)\u001b[0m\n\u001b[1;32m    465\u001b[0m                         \"answer: %s\" % (tokenization.printable_text(answer_text)))\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             feature = InputFeatures(\n\u001b[0m\u001b[1;32m    468\u001b[0m                 \u001b[0munique_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munique_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0mexample_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/bert_reader/run_squad.py\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(examples, tokenizer, max_seq_length, doc_stride, max_query_length, is_training, output_fn)\u001b[0m\n\u001b[1;32m    465\u001b[0m                         \"answer: %s\" % (tokenization.printable_text(answer_text)))\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m             feature = InputFeatures(\n\u001b[0m\u001b[1;32m    468\u001b[0m                 \u001b[0munique_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munique_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m                 \u001b[0mexample_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexample_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_writer = FeatureWriter(\n",
    "        filename=os.path.join('./checkpoints/squad_1.1_base/', \"train.tf_record\"),\n",
    "        is_training=True)\n",
    "convert_examples_to_features(\n",
    "        examples=train_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=384,\n",
    "        doc_stride=123,\n",
    "        max_query_length=64,\n",
    "        is_training=True,\n",
    "        output_fn=train_writer.process_feature)\n",
    "train_writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
