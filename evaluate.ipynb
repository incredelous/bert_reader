{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.5/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.5/dist-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.26867987646614e-234"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# caculate bleu-4\n",
    "import nltk\n",
    "import json\n",
    "list_of_references, list_of_hypothesis  = [], []\n",
    "with open('./checkpoints/jiankangyun/predictions.json', 'r', encoding='utf-8') as f:\n",
    "    test_result = json.loads(f.read())\n",
    "with open('./data/jiankangyun/mini_forum.json', 'r', encoding=\"utf-8\") as f:\n",
    "    ref_json = json.loads(f.read())\n",
    "for qas_id in ref_json.keys():\n",
    "    correct_id = ref_json[qas_id]['Correct_ID']\n",
    "    ref_texts = ref_json[qas_id]['Ans'][correct_id]\n",
    "    reference, hypothesis = [list(ref_texts)], list(test_result[qas_id])\n",
    "    list_of_references.append(reference)\n",
    "    list_of_hypothesis.append(hypothesis)\n",
    "nltk.translate.bleu_score.corpus_bleu(list_of_references, list_of_hypothesis,weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=None)\n",
    "# bleu-4 for weights=(0.25, 0.25, 0.25, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation with Avg\n",
      "\trouge-1:\tP:  4.35\tR:  3.49\tF1:  3.59\n",
      "\trouge-2:\tP:  1.16\tR:  0.99\tF1:  1.02\n",
      "\trouge-3:\tP:  0.21\tR:  0.16\tF1:  0.17\n",
      "\trouge-4:\tP:  0.09\tR:  0.08\tF1:  0.08\n",
      "\trouge-l:\tP:  4.39\tR:  3.59\tF1:  3.71\n",
      "\trouge-w:\tP:  4.33\tR:  3.22\tF1:  3.39\n",
      "Evaluation with Best\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f024cca66c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mlist_of_hypothesis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqas_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_hypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_of_references\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/rouge/rouge.py\u001b[0m in \u001b[0;36mget_scores\u001b[0;34m(self, hypothesis, references)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mhas_rouge_w_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhas_rouge_w_metric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_scores_rouge_l_or_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypothesis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/rouge/rouge.py\u001b[0m in \u001b[0;36m_get_scores_rouge_l_or_w\u001b[0;34m(self, all_hypothesis, all_references, use_w)\u001b[0m\n\u001b[1;32m    626\u001b[0m                             \u001b[0mreference_count_for_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference_count\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                             \u001b[0moverlapping_ngrams_for_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverlapping_ngrams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m                             \u001b[0mscore_wlcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moverlapping_ngrams_for_score\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mreference_count_for_score\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mbest_current_score_wlcs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mscore_wlcs\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_current_score_wlcs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "# caculate rouge, but use best aggergator occurs some error\n",
    "import rouge\n",
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "\n",
    "for aggregator in ['Avg', 'Best', 'Individual']:\n",
    "    print('Evaluation with {}'.format(aggregator))\n",
    "    apply_avg = aggregator == 'Avg'\n",
    "    apply_best = aggregator == 'Best'\n",
    "\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                            max_n=4,\n",
    "                            limit_length=False,\n",
    "                            length_limit=400,\n",
    "                            length_limit_type='bytes',\n",
    "                            apply_avg=apply_avg,\n",
    "                            apply_best=apply_best,\n",
    "                            alpha=0.5, # Default F1_score\n",
    "                            weight_factor=1.2,\n",
    "                            stemming=True)\n",
    "\n",
    "\n",
    "    list_of_references, list_of_hypothesis  = [], []\n",
    "    with open('./checkpoints/jiankangyun/predictions.json', 'r', encoding='utf-8') as f:\n",
    "        test_result = json.loads(f.read())\n",
    "    with open('./data/jiankangyun/mini_forum.json', 'r', encoding=\"utf-8\") as f:\n",
    "        ref_json = json.loads(f.read())\n",
    "    for qas_id in ref_json.keys():\n",
    "        correct_id = ref_json[qas_id]['Correct_ID']\n",
    "        ref_texts = ref_json[qas_id]['Ans'][correct_id]\n",
    "        list_of_references.append(ref_texts)\n",
    "        list_of_hypothesis.append(test_result[qas_id])\n",
    "\n",
    "    scores = evaluator.get_scores(list_of_hypothesis, list_of_references)\n",
    "\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./checkpoints/CIPS-sogou-unfactoid/predictions.json', 'r', encoding='utf-8') as f:\n",
    "    test_result = json.loads(f.read())\n",
    "for key, value in test_result.items():\n",
    "    print(key, value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
