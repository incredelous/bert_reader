{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import modeling\n",
    "from run_squad import FLAGS, convert_examples_to_features, SquadExample, write_predictions, input_fn_builder, model_fn_builder, FLAGS, validate_flags_or_throw, FeatureWriter, RawResult, get_final_text, _get_best_indexes, _compute_softmax\n",
    "import tokenization\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "FLAGS.bert_config_file = './checkpoints/chinese_L-12_H-768_A-12/bert_config.json'\n",
    "FLAGS.vocab_file = './checkpoints/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "FLAGS.init_checkpoint = './checkpoints/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "FLAGS.output_dir = './checkpoints/CIPS/'\n",
    "FLAGS.predict_file = './data/CIPS-sogou/valid.json'\n",
    "FLAGS.do_predict = True\n",
    "FLAGS.train_file = './data/CIPS-sogou/train.v1.json'\n",
    "FLAGS.do_train = True\n",
    "FLAGS.doc_stride = 256\n",
    "FLAGS.max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cips_unfactoid_examples(input_file, is_training):\n",
    "    \"\"\"Read a cips unfactoid json file into a list of SquadExample.\"\"\"\n",
    "    input_data = []\n",
    "    with tf.gfile.Open(input_file, \"r\") as reader:\n",
    "        for line in reader:\n",
    "            result = json.loads(line.strip())\n",
    "            input_data.append(result)\n",
    "\n",
    "    def is_whitespace(c):\n",
    "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    examples = []\n",
    "    for entry in tqdm(input_data):\n",
    "        paragraphs = {}\n",
    "        qas_id = entry[\"query_id\"]\n",
    "        question_text = entry[\"query\"]\n",
    "        for paragraph in entry[\"passages\"]:\n",
    "            passage_id = paragraph[\"passage_id\"]\n",
    "            start_position = -1\n",
    "            end_position = -1\n",
    "            orig_answer_text = \"\"\n",
    "            is_impossible = False\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph[\"passage_text\"]:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c)\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "            not_find_answer_on_passage = False\n",
    "            for answer in entry[\"answer\"]:\n",
    "                answer_id = answer[\"answer_id\"]\n",
    "                from_passage = answer[\"from_passage\"]\n",
    "                if from_passage == passage_id:\n",
    "                    if is_training:\n",
    "                        if FLAGS.version_2_with_negative:\n",
    "                            is_impossible = entry[\"is_impossible\"]\n",
    "                        if (len(entry[\"answer\"]) < 1) and (not is_impossible):\n",
    "                            raise ValueError(\n",
    "                                \"For training, each question should have exactly 1 answer.\")\n",
    "                        if not is_impossible:\n",
    "                            if not re.search(re.escape(answer[\"answer_text\"]), paragraph[\"passage_text\"]):\n",
    "#                                 tf.logging.warning('can not find answer from corresponding paragraph: {}\\t{}'.format(answer[\"answer_text\"], paragraph[\"passage_text\"]))\n",
    "#                                 tf.logging.warning('can not find answer from corresponding paragraph: {}\\t{}'.format(qas_id, passage_id))\n",
    "                                not_find_answer_on_passage = True\n",
    "                                continue\n",
    "                            answer_offset = re.search(re.escape(answer[\"answer_text\"]), paragraph[\"passage_text\"]).span()[0]\n",
    "                            orig_answer_text = answer[\"answer_text\"]\n",
    "                            answer_length = len(orig_answer_text)\n",
    "                            start_position = char_to_word_offset[answer_offset]\n",
    "                            end_position = char_to_word_offset[answer_offset + answer_length -\n",
    "                                                               1]\n",
    "                            # Only add answers where the text can be exactly recovered from the\n",
    "                            # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "                            # stuff so we will just skip the example.\n",
    "                            #\n",
    "                            # Note that this means for training mode, every example is NOT\n",
    "                            # guaranteed to be preserved.\n",
    "                            # 进行一些适合中文的预处理\n",
    "                            actual_text = \" \".join(\n",
    "                                doc_tokens[start_position:(end_position + 1)])\n",
    "                            cleaned_answer_text = \" \".join(\n",
    "                                tokenization.whitespace_tokenize(orig_answer_text))\n",
    "                            if actual_text.find(cleaned_answer_text) == -1:\n",
    "                                tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "                                                   actual_text, cleaned_answer_text)\n",
    "                                not_find_answer_on_passage = True\n",
    "                                continue\n",
    "                        else:\n",
    "                            start_position = -1\n",
    "                            end_position = -1\n",
    "                            orig_answer_text = \"\"\n",
    "                    break\n",
    "            if not_find_answer_on_passage:\n",
    "                continue\n",
    "            example = SquadExample(\n",
    "                qas_id=\"{}-{}\".format(qas_id, passage_id),\n",
    "                question_text=question_text,\n",
    "                doc_tokens=doc_tokens,\n",
    "                orig_answer_text=orig_answer_text,\n",
    "                start_position=start_position,\n",
    "                end_position=end_position,\n",
    "                is_impossible=is_impossible)\n",
    "            examples.append(example)\n",
    "    return examples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 2 column 1 (char 2106)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5705f1e8546c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mtrain_examples_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 2 column 1 (char 2106)"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "validate_flags_or_throw(bert_config)\n",
    "\n",
    "tf.gfile.MakeDirs(FLAGS.output_dir)\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "tpu_cluster_resolver = None\n",
    "if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "\n",
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    master=FLAGS.master,\n",
    "    model_dir=FLAGS.output_dir,\n",
    "    save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "        num_shards=FLAGS.num_tpu_cores,\n",
    "        per_host_input_for_training=is_per_host))\n",
    "\n",
    "train_examples = None\n",
    "num_train_steps = None\n",
    "num_warmup_steps = None\n",
    "if FLAGS.do_train:\n",
    "    if not tf.gfile.Exists(os.path.join(FLAGS.output_dir, \"train.tf_record\")):\n",
    "        train_examples = read_cips_unfactoid_examples(\n",
    "            input_file=FLAGS.train_file, is_training=True)\n",
    "        num_train_steps = int(\n",
    "            len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "\n",
    "        # Pre-shuffle the input to avoid having to make a very large shuffle\n",
    "        # buffer in in the `input_fn`.\n",
    "        rng = random.Random(12345)\n",
    "        rng.shuffle(train_examples)\n",
    "\n",
    "        train_writer = FeatureWriter(\n",
    "            filename=os.path.join(FLAGS.output_dir, \"train.tf_record\"),\n",
    "            is_training=True)\n",
    "        convert_examples_to_features(\n",
    "            examples=train_examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=FLAGS.max_seq_length,\n",
    "            doc_stride=FLAGS.doc_stride,\n",
    "            max_query_length=FLAGS.max_query_length,\n",
    "            is_training=True,\n",
    "            output_fn=train_writer.process_feature)\n",
    "        train_writer.close()\n",
    "\n",
    "        train_filename = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
    "        train_examples_number = len(train_examples)\n",
    "        del train_examples\n",
    "    else:\n",
    "        with tf.gfile.Open(FLAGS.train_file, \"r\") as reader:\n",
    "            input_data = json.load(reader)[\"data\"]\n",
    "        train_examples_number = 0\n",
    "        for entry in input_data:\n",
    "            for paragraph in entry[\"paragraphs\"]:\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    train_examples_number += 1\n",
    "        num_train_steps = int(\n",
    "            train_examples_number / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "        train_filename = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    init_checkpoint=FLAGS.init_checkpoint,\n",
    "    learning_rate=FLAGS.learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    use_tpu=FLAGS.use_tpu,\n",
    "    use_one_hot_embeddings=FLAGS.use_tpu)\n",
    "\n",
    "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "# or GPU.\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=FLAGS.use_tpu,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=FLAGS.train_batch_size,\n",
    "    predict_batch_size=FLAGS.predict_batch_size)\n",
    "\n",
    "if FLAGS.do_train:\n",
    "    # We write to a temporary file to avoid storing very large constant tensors\n",
    "    # in memory.\n",
    "\n",
    "    tf.logging.info(\"***** Running training *****\")\n",
    "    tf.logging.info(\"  Num orig examples = %d\", train_examples_number)\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
    "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "\n",
    "    train_input_fn = input_fn_builder(\n",
    "        input_file=train_filename,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=True,\n",
    "        drop_remainder=True)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "if FLAGS.do_predict:\n",
    "    eval_examples = read_cips_unfactoid_examples(\n",
    "        input_file=FLAGS.predict_file, is_training=False)\n",
    "\n",
    "    eval_writer = FeatureWriter(\n",
    "        filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n",
    "        is_training=False)\n",
    "    eval_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "        eval_features.append(feature)\n",
    "        eval_writer.process_feature(feature)\n",
    "\n",
    "    convert_examples_to_features(\n",
    "        examples=eval_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=FLAGS.max_seq_length,\n",
    "        doc_stride=FLAGS.doc_stride,\n",
    "        max_query_length=FLAGS.max_query_length,\n",
    "        is_training=False,\n",
    "        output_fn=append_feature)\n",
    "    eval_writer.close()\n",
    "\n",
    "    tf.logging.info(\"***** Running predictions *****\")\n",
    "    tf.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "    tf.logging.info(\"  Num split examples = %d\", len(eval_features))\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    predict_input_fn = input_fn_builder(\n",
    "        input_file=eval_writer.filename,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    # If running eval on the TPU, you will need to specify the number of\n",
    "    # steps.\n",
    "    all_results = []\n",
    "    for result in estimator.predict(\n",
    "            predict_input_fn, yield_single_examples=True):\n",
    "        if len(all_results) % 1000 == 0:\n",
    "            tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
    "        unique_id = int(result[\"unique_ids\"])\n",
    "        start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
    "        end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
    "        all_results.append(\n",
    "            RawResult(\n",
    "                unique_id=unique_id,\n",
    "                start_logits=start_logits,\n",
    "                end_logits=end_logits))\n",
    "\n",
    "    output_prediction_file = os.path.join(\n",
    "        FLAGS.output_dir, \"predictions.json\")\n",
    "    output_nbest_file = os.path.join(\n",
    "        FLAGS.output_dir, \"nbest_predictions.json\")\n",
    "    output_null_log_odds_file = os.path.join(\n",
    "        FLAGS.output_dir, \"null_odds.json\")\n",
    "\n",
    "    write_predictions(eval_examples, eval_features, all_results,\n",
    "                      FLAGS.n_best_size, FLAGS.max_answer_length,\n",
    "                      FLAGS.do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
