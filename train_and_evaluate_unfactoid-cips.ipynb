{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b764cf512dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "import modeling\n",
    "from run_squad import FLAGS, convert_examples_to_features, SquadExample, write_predictions, input_fn_builder, model_fn_builder, FLAGS, validate_flags_or_throw, FeatureWriter, RawResult, get_final_text, _get_best_indexes, _compute_softmax\n",
    "import tokenization\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "FLAGS.bert_config_file = './checkpoints/chinese_L-12_H-768_A-12/bert_config.json'\n",
    "FLAGS.vocab_file = './checkpoints/chinese_L-12_H-768_A-12/vocab.txt'\n",
    "FLAGS.init_checkpoint = './checkpoints/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
    "FLAGS.output_dir = './checkpoints/CIPS/'\n",
    "FLAGS.predict_file = './data/CIPS-sogou/valid.json'\n",
    "FLAGS.do_predict = True\n",
    "FLAGS.train_file = './data/CIPS-sogou/train.v1.json'\n",
    "FLAGS.do_train = True\n",
    "FLAGS.doc_stride = 256\n",
    "FLAGS.max_seq_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cips_unfactoid_examples(input_file, is_training):\n",
    "    \"\"\"Read a cips unfactoid json file into a list of SquadExample.\"\"\"\n",
    "    input_data = []\n",
    "    with tf.gfile.Open(input_file, \"r\") as reader:\n",
    "        for line in reader:\n",
    "            result = json.loads(line.strip())\n",
    "            input_data.append(result)\n",
    "\n",
    "    def is_whitespace(c):\n",
    "        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    examples = []\n",
    "    for entry in tqdm(input_data):\n",
    "        paragraphs = {}\n",
    "        qas_id = entry[\"query_id\"]\n",
    "        question_text = entry[\"query\"]\n",
    "        for paragraph in entry[\"passages\"]:\n",
    "            passage_id = paragraph[\"passage_id\"]\n",
    "            start_position = -1\n",
    "            end_position = -1\n",
    "            orig_answer_text = \"\"\n",
    "            is_impossible = False\n",
    "            char_to_word_offset = []\n",
    "            prev_is_whitespace = True\n",
    "            for c in paragraph[\"passage_text\"]:\n",
    "                if is_whitespace(c):\n",
    "                    prev_is_whitespace = True\n",
    "                else:\n",
    "                    if prev_is_whitespace:\n",
    "                        doc_tokens.append(c)\n",
    "                    else:\n",
    "                        doc_tokens[-1] += c\n",
    "                    prev_is_whitespace = False\n",
    "                char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "            not_find_answer_on_passage = False\n",
    "            for answer in entry[\"answer\"]:\n",
    "                answer_id = answer[\"answer_id\"]\n",
    "                from_passage = answer[\"from_passage\"]\n",
    "                if from_passage == passage_id:\n",
    "                    if is_training:\n",
    "                        if FLAGS.version_2_with_negative:\n",
    "                            is_impossible = entry[\"is_impossible\"]\n",
    "                        if (len(entry[\"answer\"]) < 1) and (not is_impossible):\n",
    "                            raise ValueError(\n",
    "                                \"For training, each question should have exactly 1 answer.\")\n",
    "                        if not is_impossible:\n",
    "                            if not re.search(re.escape(answer[\"answer_text\"]), paragraph[\"passage_text\"]):\n",
    "#                                 tf.logging.warning('can not find answer from corresponding paragraph: {}\\t{}'.format(answer[\"answer_text\"], paragraph[\"passage_text\"]))\n",
    "#                                 tf.logging.warning('can not find answer from corresponding paragraph: {}\\t{}'.format(qas_id, passage_id))\n",
    "                                not_find_answer_on_passage = True\n",
    "                                continue\n",
    "                            answer_offset = re.search(re.escape(answer[\"answer_text\"]), paragraph[\"passage_text\"]).span()[0]\n",
    "                            orig_answer_text = answer[\"answer_text\"]\n",
    "                            answer_length = len(orig_answer_text)\n",
    "                            start_position = char_to_word_offset[answer_offset]\n",
    "                            end_position = char_to_word_offset[answer_offset + answer_length -\n",
    "                                                               1]\n",
    "                            # Only add answers where the text can be exactly recovered from the\n",
    "                            # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "                            # stuff so we will just skip the example.\n",
    "                            #\n",
    "                            # Note that this means for training mode, every example is NOT\n",
    "                            # guaranteed to be preserved.\n",
    "                            # 进行一些适合中文的预处理\n",
    "                            actual_text = \" \".join(\n",
    "                                doc_tokens[start_position:(end_position + 1)])\n",
    "                            cleaned_answer_text = \" \".join(\n",
    "                                tokenization.whitespace_tokenize(orig_answer_text))\n",
    "                            if actual_text.find(cleaned_answer_text) == -1:\n",
    "                                tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "                                                   actual_text, cleaned_answer_text)\n",
    "                                not_find_answer_on_passage = True\n",
    "                                continue\n",
    "                        else:\n",
    "                            start_position = -1\n",
    "                            end_position = -1\n",
    "                            orig_answer_text = \"\"\n",
    "                    break\n",
    "            if not_find_answer_on_passage:\n",
    "                continue\n",
    "            example = SquadExample(\n",
    "                qas_id=\"{}-{}\".format(qas_id, passage_id),\n",
    "                question_text=question_text,\n",
    "                doc_tokens=doc_tokens,\n",
    "                orig_answer_text=orig_answer_text,\n",
    "                start_position=start_position,\n",
    "                end_position=end_position,\n",
    "                is_impossible=is_impossible)\n",
    "            examples.append(example)\n",
    "    return examples    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training,\n",
    "                                 output_fn):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    unique_id = 1000000000\n",
    "\n",
    "    for (example_index, example) in enumerate(tqdm(examples)):\n",
    "        query_tokens = tokenizer.tokenize(example.question_text)\n",
    "\n",
    "        if len(query_tokens) > max_query_length:\n",
    "            query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "        tok_to_orig_index = []\n",
    "        orig_to_tok_index = []\n",
    "        all_doc_tokens = []\n",
    "        # tok_to_orig_index 指示tokenize后当前的token属于第几个原始词\n",
    "        # orig_to_tok_index 指示tokenize后原始词来自于token列表(all_doc_tokens)的哪些词(连续的)，把这些词的开始位置作为值记录下来\n",
    "        # all_doc_tokens tokenize doc_tokens后获得的token列表，而doc_tokens是context whitespace_tokenize后的结果\n",
    "        for (i, token) in enumerate(example.doc_tokens):\n",
    "            orig_to_tok_index.append(len(all_doc_tokens))\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "                tok_to_orig_index.append(i)\n",
    "                all_doc_tokens.append(sub_token)\n",
    "\n",
    "        tok_start_position = None\n",
    "        tok_end_position = None\n",
    "        if is_training and example.is_impossible:\n",
    "            tok_start_position = -1\n",
    "            tok_end_position = -1\n",
    "        if is_training and not example.is_impossible:\n",
    "            tok_start_position = orig_to_tok_index[example.start_position]\n",
    "            if example.end_position < len(example.doc_tokens) - 1:\n",
    "                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "            else:\n",
    "                tok_end_position = len(all_doc_tokens) - 1\n",
    "            (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "                all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "                example.orig_answer_text)\n",
    "\n",
    "        # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "        # We can have documents that are longer than the maximum sequence length.\n",
    "        # To deal with this we do a sliding window approach, where we take chunks\n",
    "        # of the up to our max length with a stride of `doc_stride`.\n",
    "        _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "            \"DocSpan\", [\"start\", \"length\"])\n",
    "        doc_spans = []\n",
    "        start_offset = 0\n",
    "        # doc_spans 记录由doc_stride分割的context\n",
    "        while start_offset < len(all_doc_tokens):\n",
    "            length = len(all_doc_tokens) - start_offset\n",
    "            if length > max_tokens_for_doc:\n",
    "                length = max_tokens_for_doc\n",
    "            doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "            if start_offset + length == len(all_doc_tokens):\n",
    "                break\n",
    "            start_offset += min(length, doc_stride)\n",
    "\n",
    "        for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "            tokens = []\n",
    "            token_to_orig_map = {}\n",
    "            token_is_max_context = {}\n",
    "            segment_ids = []\n",
    "            tokens.append(\"[CLS]\")\n",
    "            segment_ids.append(0)\n",
    "            for token in query_tokens:\n",
    "                tokens.append(token)\n",
    "                segment_ids.append(0)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(0)\n",
    "\n",
    "            for i in range(doc_span.length):\n",
    "                split_token_index = doc_span.start + i\n",
    "                token_to_orig_map[len(\n",
    "                    tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                       split_token_index)\n",
    "                token_is_max_context[len(tokens)] = is_max_context\n",
    "                tokens.append(all_doc_tokens[split_token_index])\n",
    "                segment_ids.append(1)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            segment_ids.append(1)\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            while len(input_ids) < max_seq_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                segment_ids.append(0)\n",
    "\n",
    "            # tokens 记录形式[[cls], [query_tokens], [SEP], [context_tokens], [SEP]]\n",
    "            # input_ids 记录形式[xxx, ..., xxx, 0, ..., 0], 0 代表 padding tokens\n",
    "            # segment_ids 记录形式[0, ..., 0, 1, ..., 1, 0, ..., 0],只有 context_tokens中的token是1\n",
    "            # input_mask 记录形式[1, ..., 1, 0, ..., 0], 1 for real tokens and 0 for padding tokens\n",
    "\n",
    "            assert len(input_ids) == max_seq_length\n",
    "            assert len(input_mask) == max_seq_length\n",
    "            assert len(segment_ids) == max_seq_length\n",
    "\n",
    "            start_position = None\n",
    "            end_position = None\n",
    "            if is_training and not example.is_impossible:\n",
    "                # For training, if our document chunk does not contain an annotation\n",
    "                # we throw it out, since there is nothing to predict.\n",
    "                doc_start = doc_span.start\n",
    "                doc_end = doc_span.start + doc_span.length - 1\n",
    "                out_of_span = False\n",
    "                if not (tok_start_position >= doc_start and\n",
    "                        tok_end_position <= doc_end):\n",
    "                    out_of_span = True\n",
    "                # 如果answer的起始位置不在同一个span中，start_position 和 end_position 均置为0\n",
    "                if out_of_span:\n",
    "                    start_position = 0\n",
    "                    end_position = 0\n",
    "                else:\n",
    "                    doc_offset = len(query_tokens) + 2\n",
    "                    start_position = tok_start_position - doc_start + doc_offset\n",
    "                    end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "            if is_training and example.is_impossible:\n",
    "                start_position = 0\n",
    "                end_position = 0\n",
    "\n",
    "            if example_index < 20:\n",
    "                tf.logging.info(\"*** Example ***\")\n",
    "                tf.logging.info(\"unique_id: %s\" % (unique_id))\n",
    "                tf.logging.info(\"example_index: %s\" % (example_index))\n",
    "                tf.logging.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "                tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "                    [tokenization.printable_text(x) for x in tokens]))\n",
    "                tf.logging.info(\"token_to_orig_map: %s\" % \" \".join(\n",
    "                    [\"%d:%d\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n",
    "                tf.logging.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "                    \"%d:%s\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n",
    "                ]))\n",
    "                tf.logging.info(\"input_ids: %s\" %\n",
    "                                \" \".join([str(x) for x in input_ids]))\n",
    "                tf.logging.info(\n",
    "                    \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "                tf.logging.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "                if is_training and example.is_impossible:\n",
    "                    tf.logging.info(\"impossible example\")\n",
    "                if is_training and not example.is_impossible:\n",
    "                    answer_text = \" \".join(\n",
    "                        tokens[start_position:(end_position + 1)])\n",
    "                    tf.logging.info(\"start_position: %d\" % (start_position))\n",
    "                    tf.logging.info(\"end_position: %d\" % (end_position))\n",
    "                    tf.logging.info(\n",
    "                        \"answer: %s\" % (tokenization.printable_text(answer_text)))\n",
    "\n",
    "            feature = InputFeatures(\n",
    "                unique_id=unique_id,\n",
    "                example_index=example_index,\n",
    "                doc_span_index=doc_span_index,\n",
    "                tokens=tokens,\n",
    "                token_to_orig_map=token_to_orig_map,\n",
    "                token_is_max_context=token_is_max_context,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                segment_ids=segment_ids,\n",
    "                start_position=start_position,\n",
    "                end_position=end_position,\n",
    "                is_impossible=example.is_impossible)\n",
    "\n",
    "            # Run callback\n",
    "            output_fn(feature)\n",
    "\n",
    "            unique_id += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "validate_flags_or_throw(bert_config)\n",
    "\n",
    "tf.gfile.MakeDirs(FLAGS.output_dir)\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "tpu_cluster_resolver = None\n",
    "if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "\n",
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    master=FLAGS.master,\n",
    "    model_dir=FLAGS.output_dir,\n",
    "    save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "        num_shards=FLAGS.num_tpu_cores,\n",
    "        per_host_input_for_training=is_per_host))\n",
    "\n",
    "train_examples = None\n",
    "num_train_steps = None\n",
    "num_warmup_steps = None\n",
    "if FLAGS.do_train:\n",
    "    if not tf.gfile.Exists(os.path.join(FLAGS.output_dir, \"train.tf_record\")):\n",
    "        train_examples = read_cips_unfactoid_examples(\n",
    "            input_file=FLAGS.train_file, is_training=True)\n",
    "        num_train_steps = int(\n",
    "            len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "\n",
    "        # Pre-shuffle the input to avoid having to make a very large shuffle\n",
    "        # buffer in in the `input_fn`.\n",
    "        rng = random.Random(12345)\n",
    "        rng.shuffle(train_examples)\n",
    "\n",
    "        train_writer = FeatureWriter(\n",
    "            filename=os.path.join(FLAGS.output_dir, \"train.tf_record\"),\n",
    "            is_training=True)\n",
    "        convert_examples_to_features(\n",
    "            examples=train_examples,\n",
    "            tokenizer=tokenizer,\n",
    "            max_seq_length=FLAGS.max_seq_length,\n",
    "            doc_stride=FLAGS.doc_stride,\n",
    "            max_query_length=FLAGS.max_query_length,\n",
    "            is_training=True,\n",
    "            output_fn=train_writer.process_feature)\n",
    "        train_writer.close()\n",
    "\n",
    "        train_filename = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
    "        train_examples_number = len(train_examples)\n",
    "        del train_examples\n",
    "    else:\n",
    "        with tf.gfile.Open(FLAGS.train_file, \"r\") as reader:\n",
    "            input_data = json.load(reader)[\"data\"]\n",
    "        train_examples_number = 0\n",
    "        for entry in input_data:\n",
    "            for paragraph in entry[\"paragraphs\"]:\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    train_examples_number += 1\n",
    "        num_train_steps = int(\n",
    "            train_examples_number / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "        train_filename = os.path.join(FLAGS.output_dir, \"train.tf_record\")\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    init_checkpoint=FLAGS.init_checkpoint,\n",
    "    learning_rate=FLAGS.learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    use_tpu=FLAGS.use_tpu,\n",
    "    use_one_hot_embeddings=FLAGS.use_tpu)\n",
    "\n",
    "# If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "# or GPU.\n",
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=FLAGS.use_tpu,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=FLAGS.train_batch_size,\n",
    "    predict_batch_size=FLAGS.predict_batch_size)\n",
    "\n",
    "if FLAGS.do_train:\n",
    "    # We write to a temporary file to avoid storing very large constant tensors\n",
    "    # in memory.\n",
    "\n",
    "    tf.logging.info(\"***** Running training *****\")\n",
    "    tf.logging.info(\"  Num orig examples = %d\", train_examples_number)\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
    "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "\n",
    "\n",
    "    train_input_fn = input_fn_builder(\n",
    "        input_file=train_filename,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=True,\n",
    "        drop_remainder=True)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "if FLAGS.do_predict:\n",
    "    eval_examples = read_cips_unfactoid_examples(\n",
    "        input_file=FLAGS.predict_file, is_training=False)\n",
    "\n",
    "    eval_writer = FeatureWriter(\n",
    "        filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n",
    "        is_training=False)\n",
    "    eval_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "        eval_features.append(feature)\n",
    "        eval_writer.process_feature(feature)\n",
    "\n",
    "    convert_examples_to_features(\n",
    "        examples=eval_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=FLAGS.max_seq_length,\n",
    "        doc_stride=FLAGS.doc_stride,\n",
    "        max_query_length=FLAGS.max_query_length,\n",
    "        is_training=False,\n",
    "        output_fn=append_feature)\n",
    "    eval_writer.close()\n",
    "\n",
    "    tf.logging.info(\"***** Running predictions *****\")\n",
    "    tf.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "    tf.logging.info(\"  Num split examples = %d\", len(eval_features))\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    predict_input_fn = input_fn_builder(\n",
    "        input_file=eval_writer.filename,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    # If running eval on the TPU, you will need to specify the number of\n",
    "    # steps.\n",
    "    all_results = []\n",
    "    for result in estimator.predict(\n",
    "            predict_input_fn, yield_single_examples=True):\n",
    "        if len(all_results) % 1000 == 0:\n",
    "            tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
    "        unique_id = int(result[\"unique_ids\"])\n",
    "        start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
    "        end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
    "        all_results.append(\n",
    "            RawResult(\n",
    "                unique_id=unique_id,\n",
    "                start_logits=start_logits,\n",
    "                end_logits=end_logits))\n",
    "\n",
    "    output_prediction_file = os.path.join(\n",
    "        FLAGS.output_dir, \"predictions.json\")\n",
    "    output_nbest_file = os.path.join(\n",
    "        FLAGS.output_dir, \"nbest_predictions.json\")\n",
    "    output_null_log_odds_file = os.path.join(\n",
    "        FLAGS.output_dir, \"null_odds.json\")\n",
    "\n",
    "    write_predictions(eval_examples, eval_features, all_results,\n",
    "                      FLAGS.n_best_size, FLAGS.max_answer_length,\n",
    "                      FLAGS.do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
